\documentclass[12pt]{article}
\usepackage[pdftex]{color, graphicx}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{lscape}
\usepackage{hangcaption}
\usepackage{dcolumn}
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=6in
\textheight=8.75in
\topmargin=-.5in
\footskip=0.5in

\newcounter{fig}
\newcounter{tab}
\newcounter{app}
\date{}


\title{\begin{center}Statistics 5525: Homework 4\end{center}}

\begin{document}
\maketitle
\newcommand{\argmin}{\text{argmin}}
\noindent For each homework assignment, turn in at the beginning of class on the indicated due date. Late assignments will only be accepted with special permission.
Write each problem up \emph{very} neatly (\LaTeX\quad  is preferred). Show all of your work.


\bibliographystyle{bioinformatics}
\noindent
\section*{Problem 1}
Read \emph{Nonlinear Dimensionality 
Reduction by 
Locally Linear Embedding}, Sam T. Roweisand and Lawrence K. Saul, \emph{Science} (2000). 
 
Done.

\section*{Problem 2}
Read \emph{Probabilistic Principal Component Analysis}, Michael E. Tipping and Christopher M. Bishop (1999).  

Done. 

\section*{Problem 3}
Obtain the ``cereal" data set from the website.
The matrix $X' = <x_1',\dots,x_22'>$ indicates the features (Calories Protein Fat Sodium Fiber Carbo Sugars Shelf Potass Vitamins) over each of 22 cereals (names indicated at the bottom of the file).

\subsection*{Part a}
Perform 2-D Classical MDS on the data set.
That is, find lower dimensional coordinates ($z's$ in 2-D), such that the found $z's$ minimize the stress function:
\begin{equation}
\sqrt{\sum_{i< j}(||z_i-z_j||_2-||x_i-x_j ||_2)^2}.
\end{equation}
(Note: in Matlab, this is accomplished either via the cmdscale function, or the mdscale function, using Euclidean norms and the ``strain'' metric).


Here I performed 2-D Classical MDS on the cereal data set with the R package. Before doing the MDS, I rescaled the columns of the data set so that each variable has its mean as zero and standard deviation as one. the MDS mapped the data in ten dimensional feature space into a two dimensional space where the value in expression (1) is minimized. The data in the reduced space is given in Figure 1.

\begin{figure}[h!]
  \centering
  \includegraphics[width=4.5in, keepaspectratio=true]{MDS.png} 
  \caption{The result of feature space dimension reduction with Multi-dimensional Scaling (MDS).}
 \end{figure}

\subsection*{Part b}
Perform a 2-D PCA projection of the data.

Here I coded the algorithm of Principal Component Analysis. Here's the detailed procedures that I have implemented: first, I scaled the columns of the data; second calculate the covariance matrix with the scaled data; third, find the eigenvectors $V$ that diagonalize the covariance matrix; forth, collect the first two columns of $V$ (the two-column matrix is named $V_{tranc}$); fifth, transform X into a low dimensional space with X$V_{tranc}$. The data in the reduced space is given in Figure 2.

\begin{figure}[h!]
  \centering
  \includegraphics[width=4.5in, keepaspectratio=true]{PCA.png} 
  \caption{The result of feature space dimension reduction with Principal Component Analysis (PCA).}
 \end{figure}

\subsection*{Part c}
Verify that the \emph{relative} distances between those found in the 2-D projections in parts a \& b are the same. Make a conclusion.

Comparing the data points in Figure 1 and Figure 2, we can find the relative distances between the data points in Figure 1 and Figure 2 are exactly the same. More careful observation tell us the Figure 2 is just a mirror image of Figure 1 against the diagonal line from top left corner to the bottom right corner. Figure 3 is a mirror image of Figure 2, and you can see now Figure 3 is exactly the same as Figure 1. This means the classical MDS is equivalent to PCA.

\begin{figure}[h!]
  \centering
  \includegraphics[width=4.5in, keepaspectratio=true]{PCAMirror.png} 
  \caption{A mirror image of the result of feature space dimension reduction with Principal Component Analysis (PCA).}
 \end{figure}
 
\end{document}




